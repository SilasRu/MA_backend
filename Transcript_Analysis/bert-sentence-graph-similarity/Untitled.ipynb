{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb7a7e9f-6c77-4d9b-bfc2-6258c8630de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transcript_Analysis import *\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62a9c6bf-7cea-4743-8c03-32d3696870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/Users/user/Documents/dialogue_summarization/transcriptsforkeyphraseextraction'\n",
    "filename = '2021-07-12 14.35.38 Interscriber Wrapup.m4a.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dirname, filename))\n",
    "utterances = df['Utterance'].tolist()\n",
    "text = ' '.join(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaea809c-4221-435d-bdbb-c34efbe4b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e0d2b73-cd88-476e-9f46-f64106e8489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = BartModel.from_pretrained(\"facebook/bart-large\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bf6c28e-73df-4844-9727-5a76c8fbd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sentences[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8789d019-cf1e-41b3-b665-163b42a6f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '.', 'ĠHello', 'Ġeverybody', '.', 'ĠOkay', '.', 'ĠUm', 'Ġjust', 'Ġa']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sample)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1004ca4-024e-4641-b6c9-d5ddff7d2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text, max_len = 800):\n",
    "    tokenized_sents = text.split()\n",
    "    length = len(tokenized_sents)\n",
    "    num_chunks = length // max_len\n",
    "    tokenized_sents_chunks = [' '.join(tokenized_sents[i:min(length, i + max_len)]) for i in range(0, length, max_len)]\n",
    "    return tokenized_sents_chunks\n",
    "\n",
    "tokenized_sents_chunks = get_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cae90e28-f9a7-4328-ba76-806c5a9f2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2e94f07-6706-49d4-97ff-e7f11173a6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um just a remark at the beginning, I am recording this meeting because we need some templates for our next step of interscriber to summarize meetings, and we need some English samples.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = get_chunks(sample)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb9190a8-d6e9-4581-a0b3-7f8918904c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_hidden_state(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd254461-4d65-476b-b1c8-ae58d5e253e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      "Hello everybody.\n",
      "Okay.\n",
      "Um just a remark at the beginning, I am recording this meeting because we need some templates for our next step of interscriber to summarize meetings, and we need some English samples.\n",
      "So this will be one of them.\n",
      "Hopefully hope that it's okay for you.\n",
      "So how are you doing?\n",
      "Where are we?\n",
      "I suggest we make a very brief wrap-up at the beginning but but only something like 1 or 2 minutes what you have done because I would like to talk also about what we are going to do in the next weeks until September.\n",
      "So I suggest we start someone starts and then hands over to the next person.\n",
      "Maybe, since you're at the moment, the largest on my screen.\n",
      "Katia, could you start well before we begin, Mark, we discussed this morning about writing.\n",
      "These summaries of the meeting.\n",
      "Should we do this or this one, or Yeah, we can do it.\n",
      "Yeah.\n",
      "So the idea is that at the end, we all sit down and write a very brief summary of the meeting with the most important point so that we get an impression of what you think should be in the summary.\n",
      "And on the other hand, we will discuss later in the second part of the meeting, how we might generate this automatically.\n",
      "So maybe you take notes.\n",
      "Or we will also transcribe the meeting afterwards and send around the transcript so you can use this as the basis for your summary.\n",
      "Okay.\n",
      "Do you want to add something done?\n",
      "Yeah, I was just saying it.\n",
      "I think I will take some notes during the meeting already.\n",
      "Just like with the right sort of the protocol, but I think it's up to everyone else.\n",
      "And how they think the it's what the summary should look like.\n",
      "And but, I mean, this shouldn't be very, very long summer.\n",
      "It should be like one paragraph or something like that.\n",
      "It's don't give too much details.\n",
      "Because are you direct the people already?\n",
      "What you want to have?\n",
      "Maybe someone thinks well three page summary is something good.\n",
      "So yeah.\n",
      "Okay enough.\n",
      "But you can also rely on the transcript only in the aftermath, then we see if it is possible to write.\n",
      "A summary based on the transcript.\n",
      "So, okay.\n",
      "So anyway, Katya would you start.\n",
      "Sure?\n",
      "So the last week voice activity detection failed for a bit, but as I wrote like, I found the reason it was just, yeah, when it was starting the Dockers at the beginning, when I didn't really know what I was doing it was creating like like this kind of Of Orphans.\n",
      "Dockers that are not really like they're not working but they're they're they're kind of cached.\n",
      "So deleting them made a lot of space and I think it will not be any problem.\n",
      "Now then also last week I upload it on the service, on the server, the new service for the estimation of the word length.\n",
      "Which also messed up a little bit with the server and the space there at the moment.\n",
      "After the reboot of server, it's still for some reason behaves differently than the other Brokers.\n",
      "Which I don't understand why, because it has absolutely the same settings of a Docker and Docker compose and the service on the server.\n",
      "But it works is just behaves differently.\n",
      "It created a new Docker after the reboot while the old Dockers, they are not creating the all stalkers.\n",
      "I don't know why the new doctors I don't know why.\n",
      "But at the moment works and I think I will be.\n",
      "Now rather integrating it and interscriber.\n",
      "So trying to understand how the voice activity detection was integrated so that it can be integrated in the same manner.\n",
      "End.\n",
      "Yeah, and also made voice activity detection available on the GitHub, had problems with that in the past.\n",
      "But now, it seems that I managed to get it.\n",
      "And yeah, that's the plate from me.\n",
      "Okay.\n",
      "So that's so that it's clear voice.\n",
      "Activity detection, should be working.\n",
      "Absolutely fine unless something new comes up.\n",
      "Okay.\n",
      "Yeah.\n",
      "And apparently we had a large audio file transcribe.\n",
      "Something like 120 minutes and it worked so great.\n",
      "Could you hand over to the next person?\n",
      "Just?\n",
      "Yeah.\n",
      "By me.\n",
      "The next one is done here.\n",
      "Yeah.\n",
      "So what I did?\n",
      "Well I basically stress tested the vad and the on-premise terrorization and just so I know how much resources that we and need for the new integration server and I kind of have a feeling now, what we need and I think I'm going to make the first version of it in the interscriber project on the uphill.\n",
      "And I'm going to ask Remo to give us some more resources because we need them.\n",
      "Yeah that's the one thing I did also I did look further into dynatrace and I am still a fan of it but it is too expensive I believe yeah.\n",
      "So for the needs that we have, which is basically just looking at the docker containers and see if they are life and working properly.\n",
      "Then this is way too old Power that that might be handy at some later stage.\n",
      "The one thing that really hooked me was the session replay because it somehow in checks Some magic JavaScript functions into your website.\n",
      "And then you can basically replay the session of user and quickly going to share my screen.\n",
      "This is how I cannot share.\n",
      "Oh, sorry.\n",
      "Wait a second.\n",
      "Now, you should be able.\n",
      "Yes.\n",
      "Perfect.\n",
      "I'd remember two minutes per person approximately.\n",
      "Yeah.\n",
      "It won't take very long.\n",
      "The one thing that really hooked me is, as I said, a user session replay and with look somehow like this and yeah, you see everything in here when the user takes, which action Believe that super question were see but also super awesome.\n",
      "Really?\n",
      "Yeah, yeah, it's just amazing for me.\n",
      "It really helps you to see how the user.\n",
      "Behaves.\n",
      "In your.\n",
      "Tool and okay.\n",
      "And that area looking for the editor.\n",
      "Does it work for the editor?\n",
      "I do have the transcript of the file.\n",
      "You sent me.\n",
      "I'm not sure.\n",
      "Can I show this?\n",
      "Yeah.\n",
      "All right, so you see everything and you can mask this.\n",
      "So if you want to you can really should say all of those words should be just stores and yeah.\n",
      "Okay if you look here I think.\n",
      "And there it did, automatically somewhere in the beginning.\n",
      "Okay.\n",
      "So now this would be great to analyze the user Behavior.\n",
      "Yeah.\n",
      "But I think that only, that would be nice at some later stage in the project not now because yeah.\n",
      "And the reason why I really liked, it also is because it is basically your store, you install one file.\n",
      "Enter did everything.\n",
      "Okay?\n",
      "Yeah, it's.\n",
      "Very few ahead to configure nothing.\n",
      "Okay, which is creepy, but yeah.\n",
      "Okay.\n",
      "Anyways, I found some other monitoring tools that line, our prayer in the price range.\n",
      "You defined and have a similar.\n",
      "Yeah, functionalities Okay.\n",
      "Yeah.\n",
      "So did you did you lie about it?\n",
      "Or.\n",
      "I'm interscriber not as sorry.\n",
      "Dynatrace, not anymore.\n",
      "The because.\n",
      "The other ones.\n",
      "Yeah.\n",
      "Yeah.\n",
      "That one.\n",
      "I just found this morning and I really was looking for.\n",
      "Okay, what's software?\n",
      "And yeah, I'm gonna discuss it with her.\n",
      "Okay?\n",
      "Wendy, I think that everything for me.\n",
      "So can you pick the next one then?\n",
      "Yeah, Dawn it's the next one from me.\n",
      "Yeah so I only can report that we had a meeting this morning about going forward with Scott Insight or summarization of the dialogues, but I think we'll talk about that later so yeah, that's pretty much it from my side i have Union next to me.\n",
      "Okay, so I did some small things such as increasing the estimated transcription time as you ask Mark but when I checked for English for me, it was never longer than before.\n",
      "So I just left it as it was.\n",
      "Okay, try it again because maybe it was one time thing.\n",
      "Then for German it was indeed longer and crease it in one and a half times because all sign.\n",
      "Rooster, when the pain premise, chosen into X.\n",
      "So when it's German and on-premise going to show quite a large number but probably realistic.\n",
      "You can try it on integration and see if it's better.\n",
      "Now, looks better and most of the time I was working on the Microsoft Vista teabag, I was able to fix it.\n",
      "The problem was that when one field in the object was empty, very simple break.\n",
      "So it was just one liner fix.\n",
      "The problem is that we only have one Microsoft Vista team point and if I just deploy it it will just affect what's up and integration.\n",
      "So I'm trying to very slowly talk test it locally and I was only a book replaced stable pipeline for that around two hours ago.\n",
      "So I will continue testing it and if everything is okay I'll just add it.\n",
      "Today or tomorrow and hope that it will deploy his out problems.\n",
      "Okay.\n",
      "The next one for me is a Nico.\n",
      "So I'm still familiar is advising more safer with the project.\n",
      "And I've also started working on the issue with for adding deleted products, to the users listing, then had me going as a so far.\n",
      "Have I have renamed some cones and then added some new new once, and Yeah.\n",
      "Still Remains the loads occur behind the, have to see how the others are implemented them.\n",
      "Then implement the new columns accordingly.\n",
      "Hey.\n",
      "Do you need some help with the code?\n",
      "Because I think you started already week ago.\n",
      "So I wonder if you get stuck somewhere or.\n",
      "Know I started on the last Thursday with this issue, okay?\n",
      "So far I mean, I'm shaking.\n",
      "Also they what is going on, that's why it's taking longer.\n",
      "Okay, good.\n",
      "So far, I hope this morning was contacting you with Leah for everything I need.\n",
      "So I think now is going fine.\n",
      "Okay good.\n",
      "So the next one would be Nicola.\n",
      "Well, I was doing exams so not much.\n",
      "At all.\n",
      "Yeah, welcome back to life.\n",
      "So to say, how are you exams?\n",
      "Did you get ready?\n",
      "The results or no audience, so.\n",
      "Going everything, but one was fine.\n",
      "I wasn't very good in cloud computing.\n",
      "Unexpected different stuff to come up actually but the other ones were.\n",
      "Fine.\n",
      "Okay was lecturer Christoph Marty or To my spoon out.\n",
      "Yeah.\n",
      "Okay, yeah, yeah.\n",
      "Hehe makes surprising exams.\n",
      "I would assume that here.\n",
      "He wanted to know names of services, unknown services and I didn't learn them by heart.\n",
      "Okay?\n",
      "It was a written exam or an hour.\n",
      "It was it was on mood.\n",
      "But in our actual room or in the p, in the have a tent.\n",
      "Okay, here, in Zurich.\n",
      "Yeah.\n",
      "Okay, strange setting.\n",
      "Yep.\n",
      "Okay.\n",
      "Yeah, good.\n",
      "Yeah, I think you will have to come back on track and see what we are doing.\n",
      "We had some some technical issues.\n",
      "So we had to take down interscriber for two weeks, but it's now up and running again.\n",
      "And And yeah, then you can pick the next tasks from Tiger once you know what is going on, okay.\n",
      "Do I have to pick another one?\n",
      "Yeah, probably.\n",
      "I think it's only Young.\n",
      "Yes or the last two weeks, I also was on medication for writing my thesis, but since the cool thing about our job is that the algorithms learn by themselves i had speech-to-text training while I was writing so, in these days, I will see if there was any progress made the other than that, at the moment, I'm more busy with other Projects a bit so I don't know how much I can work on that i don't know i have to plan a bit so, today, I'm just coming back.\n",
      "Definitely okay with it.\n",
      "I think Allah is left, right?\n",
      "Yeah, I don't really have much to tell either.\n",
      "I was mostly busy with the Swiss style like collection so far.\n",
      "And as far as interscriber, we are now going to start preparing for the E. Th industry day and that's more or less it So far.\n",
      "But but you took some recordings for this Corpus that we are going to build.\n",
      "Well yes we took recordings and we are still waiting for recordings of you made any because while we can only make our individual recordings but I guess we need more group or like two people recordings.\n",
      "So I think it's best.\n",
      "If you write explicitly to people and say I need to have this recording and this way and So they just do it.\n",
      "mean you can also take this recording that we are taking here but if I mean, you know, best what is missing.\n",
      "So just going actively to the people and say I need to have this recording in the next week.\n",
      "Okay.\n",
      "So, thank you very much.\n",
      "I think since interscriber is now up and running again, we are back in a normal mode so we still have to fix things, but I would like to look.\n",
      "Now a little bit ahead until September what's going to happen by a summer.\n",
      "I think it would be great if we would have an impressive demo when we are at this industry Day and The Rush Event, I hope we still have this opportunity to go to rush because they didn't come back to me.\n",
      "Although I wrote one week ago, what are the details?\n",
      "But at least a tth industry day, there will be several hundred people and we will have the opportunity to show our solution.\n",
      "And we showed already last year that we can transcribe audio files.\n",
      "So I would like to show something new and we also promised for the you know Space Project that at some time, we will generate actionable insights.\n",
      "I think we called it and I think I will share my screen now so that we can briefly look at what we have in mind.\n",
      "Try to write down at least the first thing I didn't finish from today in the morning everything that would be interesting.\n",
      "So so during the summer I would like to develop three things if it is possible and you have to say if it is possible to get them running until September.\n",
      "So the main goal is to have an impressive demonstration at least two exhibitions One will be in September 6 and 1 will be in September 24 or something like that.\n",
      "So both at in September and this is what I would like to have and I will briefly explain what it is in the next minutes.\n",
      "So the first thing is texts, which presents the highlights of a meeting.\n",
      "It's kind of a summary, but not in the classic.\n",
      "Way of writing meeting minutes or summary likes abstract of summarization.\n",
      "It's more like a bullet point summaries with many interesting facts about the meeting.\n",
      "I will come to that.\n",
      "The second one is a transcript Explorer which is for people who want to know one particular thing about the transcript, like when did we talk about this exhibition in September, they want to have basically a smart way to search and Find stuff in the transcript.\n",
      "That will be a lot of user interface and and then some index in the back end to to make it efficiently searchable and third one would be to have at least some integration of Swiss German.\n",
      "We have built a solution for swiss-german, at least for Bernice and integrating this and, and showing a demo where we can transcribe something into a standard German, which is at least a reasonable transcript would be great.\n",
      "Now, I'm looking while I'm talking at particular people, so I have some people in mind who might do this, but I think let me first.\n",
      "Explain a little better.\n",
      "What?\n",
      "What is the idea?\n",
      "So when we talk about meetings, we have different types of meetings, always in mind that can be an interview of a journalist or it can be a political debate or it can be a team meeting like ours here.\n",
      "So one setting is something like this team meeting which takes usually one or half an hour or two one hour and a couple of participants between two and seven.\n",
      "And If someone participated, he will not be interested in looking up anything, but if someone missed the team meeting, he would like to know, he, or she if it is worth looking at particular segments of the meeting in more detail.\n",
      "So, the goal of these meeting highlights would be to send each person a brief summary, some five to ten lines, which says, what was the topic of the meeting?\n",
      "What?\n",
      "Happened in the meeting.\n",
      "And if this is all I would say boring then you know okay it's not interesting for me, I can skip it.\n",
      "Whereas if there is some point like oh the server crashed for three days and I don't know.\n",
      "Usually I has to restart it and that was talked about at the meeting then if she had missed the meeting she would like to know it.\n",
      "So basically this is a goal to write down some facts.\n",
      "And here are some some kind of examples.\n",
      "How it could look like this would be a rule-based system, which takes all the statistical information or NLP extracts from our transcript and generates an a set of samples and sentences and then we pick those sentences which we sent to the users so it can be something like we know which topics Custom then we generate a sense like most important topics were topic ABC but topic d and e will also discuss that sometime.\n",
      "Now, if you are interested in any of these topics you might click on a button behind and then you jump to the corresponding segments in the transcript to see what happened there and maybe you can just read these examples.\n",
      "And today in the morning, I discussed with the POs and done that it should be possible to generate these examples based on some, some standard and IP tools and statistics about the transcript.\n",
      "So the idea would be that we do it in two steps.\n",
      "One is to have in the back and a set of services which extract information structured information from the transcript like which topics were there which keywords were mentioned who spoke when what, how much and so on.\n",
      "And then we have a set of text Plates and each template says, oh I have enough information available and I am relevant for this meeting.\n",
      "And then we select in the third step, which of these sentences will be put into the summary in the highlights track.\n",
      "Okay?\n",
      "So that would be one part and for that, we need to implement some of these back-end Services which extract information from a transcript and we would go mainly for standard methods.\n",
      "It's not the idea that we Implement top new rocket science methods, but we might take for the Keywords just tf-idf or some some keyboard or whatever and use it and adapt it and optimize it a little bit for for the setting of interview.\n",
      "Transcripts for the speaker contribution, we just count how many seconds is Speaker spoke.\n",
      "And then compared to the others and say, oh this one has three minutes and the other one has 27, and we compute the ratio and so something like that.\n",
      "So this means we need to implement some back-end algorithms, and we thought that maybe Danny could do that mainly to implement all these algorithms.\n",
      "And then we need to have these templates and this is two steps.\n",
      "One is to write sample sentences, which might be interesting in such a summary I'm still calling it summary or highlights text or whatever and then writing really the template in the meaning that it extracts the information and puts data inside into it and having a framework to do that properly and we thought a lot that you could help me with writing some of these texts samples to to come up with say a hundred sentences that could be interesting about a meeting.\n",
      "so that we have a large variety ins and then we can have an algorithm which picks some of these sentences.\n",
      "And says, now these are relevant for this meeting and should be sent to the users.\n",
      "Okay, I guess we can do.\n",
      "I mean.\n",
      "I don't quite understand.\n",
      "It yet, but I guess we can still discuss it.\n",
      "Yeah.\n",
      "So the idea would be that we try to figure out what would be an interesting statement about a meeting and just write a sentence like, like, in this previous examples and if we have this sentence, like the second one speaker, one has Vaulted extensively about topic, a b, and c. Then the statistics behind would be that from the meeting, which is 60 minutes, maybe 35 minutes, were on topic a, then 20 minutes on topic be and five minutes on topic.\n",
      "See and all was done by Speaker 1:so.\n",
      "He talked about that a lot And then we Define kind of.\n",
      "So first we write these sentences and then second we defined what rules have to apply such that, such a sentence becomes relevant or fires in the sentence in the context.\n",
      "So.\n",
      "It doesn't have to be paired with with the recorded meeting is just.\n",
      "So it doesn't have to be paired, we just ride it out of the blue and then we see what kind of data we would.\n",
      "Delete, and then we see how much effort it is to gather this data.\n",
      "I mean, there is some it, mean, the coolest statement would be the main decisions were x y, and set.\n",
      "And that would require that we can detect main decisions, which is a real hard problem, which we will not solve until September.\n",
      "So this will not go into the set of Sentences that we can integrate at the moment, but I think many other things can be said about a meeting and we will try to figure out how to do that properly.\n",
      "Okay.\n",
      "Okay.\n",
      "And then I mean it will not be only the three of us but we would also ask other people to help out with it but I think we will make a first step and see how to do that probably together with Don who also knows what what is the main idea behind it, Now further smart, transcript Explorer, we don't know exactly how to do it yet properly.\n",
      "There are some ideas like I want to search for a keyword and I want to say oh or I want to see when was this keyword mentioned?\n",
      "So this is what these three students in their last Bachelor thesis did that say just draw a timeline and say, now, the key word was mentioned that these ticks and then I can click On some fraction of the timeline and sees the utterances, where the key word occurs, and maybe I can filter by the users who mentioned the keyword and stuff like that.\n",
      "It's kind of a smart search engine and I think technically it will not go too deep.\n",
      "It's just keyword finding and an index.\n",
      "But we need to have a nice front and for this which makes it easy to use.\n",
      "And I think we first have to decide what features it should he have and then implement it into interscriber as a separate tab.\n",
      "So we sketched some some weeks goes something like some other ideas but which we will try to make it as smooth as possible for the user to just type in a keyword make some suggestions And it clicks and he sees the utterances and then he can maybe click and listen to the and corresponding audio something like that.\n",
      "And I think Yulia that would be great if we could do it together to design and implement it.\n",
      "All right.\n",
      "I remember you already said it.\n",
      "Yeah, I.\n",
      "Agree.\n",
      "Okay.\n",
      "Right.\n",
      "And then the third part would be swiss-german, and now I'm basically looking at yarn who is somewhere in China, but even in the distance are, no, no thatthat should be Japan, I guess, right?\n",
      "Sorry.\n",
      "So do you think we could integrate some Swiss German transcript?\n",
      "June until September based on the model that we have.\n",
      "Let's just take what the students did and integrate that.\n",
      "Yeah.\n",
      "Yeah.\n",
      "Right.\n",
      "And you gave you the perfect score for it.\n",
      "So mean the one from.\n",
      "Well that doesn't mean that the transcripts will perfect.\n",
      "It only meant that the theory this was good.\n",
      "So.\n",
      "Yeah, I think for a demo where we can control the demoed, it should be enough.\n",
      "Okay, take that.\n",
      "I mean what I will.\n",
      "Do is take it and deploy it as I.\n",
      "And then someone on the backend, side can link it up to two interscriber and then that's it.\n",
      "I think can be pragmatic about this because I don't think I will come up with a better solution.\n",
      "Yeah, I mean I can try something but no it's not the question of if we can improve the speech-to-text engine, it's more like you said, even if you have a speech-to-text engine building, a transcription engine around which works for say different settings and audio files and audio files of arbitrary.\n",
      "Langford stuff like that.\n",
      "This is something that you are working now on for German.\n",
      "So we have to do that first with German as well.\n",
      "No, I think.\n",
      "Let her know how well do.\n",
      "I don't know how well these things that the students did our work like in production but you are talking about the demo at the at this party and there we can control a bit out.\n",
      "The audio looks like and everything.\n",
      "So I think we can How'd it and then be aware of the restrictions.\n",
      "Maybe that's something that we have to see where it restrictions lie.\n",
      "So maybe refuel speak like some kind of teach then it doesn't work laughs.\n",
      "Yeah.\n",
      "That door.\n",
      "If you talk about anything else than politics, it doesn't work.\n",
      "Yeah.\n",
      "We.\n",
      "We have to check that.\n",
      "But in general, if we, if it's only about making a demo and we can add it and then whoever is presenting it has to know about it and yeah like I'd stood a.\n",
      "Moment.\n",
      "Yeah.\n",
      "We have to fake a little bit at the demos which is usually the case of course or at least do not show everything that doesn't work this way.\n",
      "I think Allah you will make the game also you should listen already intensively to We cannot show.\n",
      "Yeah.\n",
      "Okay.\n",
      "Good.\n",
      "Yeah, this is what I want to to show and share with.\n",
      "You.\n",
      "Are there any comments?\n",
      "Questions, remarks.\n",
      "Okay, nothing maybe just from my side of me for me, everything is clear except from how do we select a top K sentences.\n",
      "So but I mean this is probably an algorithmic problem that they have to solve, but it's not clear to me yet how to do this.\n",
      "Yeah, II would say it depends on how extreme the numbers are.\n",
      "Let's let's assume you have this template for the data to them.\n",
      "With this intense debate about something between two speakers.\n",
      "Now, this is very easy to detect because we have this concept of three different types of conversation.\n",
      "One is a discussion where people have something like back and forth with short sentences and they really are crying at each other somehow.\n",
      "And then we have something like a nice debate where someone says something and someone else responses.\n",
      "So rather long.\n",
      "Sentences.\n",
      "And then we have like I did before more kind of a monologue.\n",
      "So and if it's in the discussion, maybe it's discussion is a better word.\n",
      "Then then we say a discussion only starts when it is at least two minutes.\n",
      "And then we could say the further away, the true length of the discussion is the more relevant is to be displayed in the summary.\n",
      "So if you were say, the minimum length is 2 minutes, and if it is 10 minutes, then it gets a higher score.\n",
      "And if it is 20 minutes of discussion between two persons out of 7, then it gets an even higher score because it's more relevant for the summary.\n",
      "And this way, we Define a rule set for every sentence to identify how relevant its outcome is okay.\n",
      "So we will also Implement some sort of scoring Scheme to say, okay, a set of scoring.\n",
      "So to say, every sentence knows something like a score between 0 and 1 and says, oh, I'm rather important.\n",
      "And I'm very important and, and this way, we have the very important ones and then we select them randomly or something like that.\n",
      "Not really clear how to do that for each sentence, but I think the principle could be, could be easy to implement.\n",
      "Okay.\n",
      "Yeah, if everything is clear, I think we can start working on this.\n",
      "I will try to cut get in touch with you which each of you and and discuss what you can do.\n",
      "I mean some people like Nicolas and and Niko didn't were not mentioned yet, but you will also be involved in this of course.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk)\n\u001b[0;32m----> 6\u001b[0m     last_hidden_states[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mget_last_hidden_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mget_last_hidden_state\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_last_hidden_state\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:1211\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1205\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1206\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1207\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1208\u001b[0m     )\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1211\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:1070\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1059\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   1060\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1067\u001b[0m     )\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:401\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    399\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    409\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:266\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# partitioned aross GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 266\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights_reshaped, past_key_value\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/modules/linear.py:94\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/general/lib/python3.8/site-packages/torch/nn/functional.py:1753\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# chunks = get_chunks(text)\n",
    "chunks = sentences\n",
    "last_hidden_states = np.zeros((len(chunks), 1024))\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(chunk)\n",
    "    last_hidden_states[i] = np.mean(get_last_hidden_state(chunk), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f9075-9daf-407c-adc9-9cb28a520f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:general]",
   "language": "python",
   "name": "conda-env-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
